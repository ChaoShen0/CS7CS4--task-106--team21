{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-22dc44a0b9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_vis(imgs):\n",
    "    \"\"\"takes a list of images and show them side-by-side in a grid\"\"\"\n",
    "    nh = nw = int(np.ceil(np.sqrt(len(imgs))))\n",
    "    h, w = imgs[0].shape\n",
    "    grid = np.zeros((nh*h, nw*w))\n",
    "    for n, img in enumerate(imgs):\n",
    "        i, j = n%nh, n//nh\n",
    "        grid[j*h:(j+1)*h, i*w:(i+1)*w] = img\n",
    "    return grid\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    reshape vectors back to images\n",
    "    pad with 2 zero pixel border to make shapes nice\n",
    "    rescale values to 0 mean -1 to 1 range (from usual 0 to 1)\n",
    "    \"\"\"\n",
    "    x = x.reshape(-1, 28, 28)\n",
    "    x = np.pad(x, [[0, 0], [2, 2], [2, 2]], mode='constant', constant_values=0)\n",
    "    return x[:, :, :, np.newaxis]*2-1\n",
    "\n",
    "def dense(x, scope, n_h):\n",
    "    \"\"\"\n",
    "    standard affine layer\n",
    "    scope = name tf variable scope\n",
    "    n_h   = number of hidden units\n",
    "    n_x = number of input to neural unit\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w', [n_x, n_h], initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [n_h], initializer=tf.constant_initializer(0))\n",
    "        return tf.matmul(x, w)+b\n",
    "    \n",
    "def conv(x, scope, r_f, n_f, stride=1):\n",
    "    \"\"\"\n",
    "    standard conv layer\n",
    "    scope  = name tf variable scope\n",
    "    r_f    = receptive field of kernel\n",
    "    n_f    = # of kernels\n",
    "    stride = locations\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w', [r_f, r_f, n_x, n_f], initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [n_f], initializer=tf.constant_initializer(0))\n",
    "        return tf.nn.convolution(x, w, padding='SAME', strides=[stride, stride])+b\n",
    "\n",
    "def upsample(x, ratio=2):\n",
    "    \"\"\"\n",
    "    takes a 4D image tensor and increases spatial resolution by replicating values\n",
    "    so for ratio=2 a 2x2 image becomes a 4x4 image with each value repeated twice\n",
    "    ratio = # of spatial repeats\n",
    "    \"\"\"\n",
    "    n_h, n_w = x.get_shape().as_list()[1:3]\n",
    "    return tf.image.resize_nearest_neighbor(x, [n_h*ratio, n_w*ratio])\n",
    "    \n",
    "def generator(Z, reuse=False):\n",
    "    \"\"\"\n",
    "    modified DCGAN (progressive GAN-ized)\n",
    "    consists of interleaved upsample and convolution operations\n",
    "    leaky relu used for good gradient flow\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        h = tf.nn.leaky_relu(dense(Z, 'hz', n_h=256*4*4), 0.2)\n",
    "        h = tf.reshape(h, [n_batch, 4, 4, 256]) #convert from feature vector to 4D image tensor\n",
    "        h = upsample(h)\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h', r_f=5, n_f=128), 0.2)\n",
    "        h = upsample(h)\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h2', r_f=5, n_f=64), 0.2)\n",
    "        h = upsample(h)\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h3', r_f=5, n_f=32), 0.2)\n",
    "        x = conv(h, 'hx', r_f=1, n_f=n_c) #linear output - could use sigmoid or tanh but gradient vanishing issues\n",
    "        return x\n",
    "    \n",
    "def discriminator(X, reuse=False):\n",
    "    \"\"\"\n",
    "    DCGAN disriminator without batchnorm for simplicity\n",
    "    leaky relu used for good gradient flow + smoother function\n",
    "    keep model shallow and simple with quick downsample via strided convolutions\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        h = tf.nn.leaky_relu(conv(X, 'hx', r_f=5, n_f=32, stride=2), 0.2)\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h', r_f=5, n_f=64, stride=2), 0.2)\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h2', r_f=5, n_f=128, stride=2), 0.2)\n",
    "        h = tf.reshape(h, [n_batch, -1]) #convert from 4D image tensor to feature vector\n",
    "        logits = dense(h, 'clf', 1)\n",
    "        return logits\n",
    "    \n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    default GAN training loss\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "def noise(n, n_z):\n",
    "    \"\"\"\n",
    "    n random spherical gaussian noise with unit variance\n",
    "    \"\"\"\n",
    "    return np.random.randn(n, n_z).astype(np.float32)\n",
    "\n",
    "def visualize(z):\n",
    "    \"\"\"\n",
    "    show evolution of samples of the same points in latent space over training\n",
    "    \"\"\"\n",
    "    g_z_sample = sess.run(g_z, {Z:z})\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.title('G: %.3f D: %.3f Updates: %d'%(g_cost, d_cost, n_updates))\n",
    "    plt.imshow(grid_vis(np.clip((g_z_sample.reshape(-1, 32, 32)+1)/2, 0, 1)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "def PlotComparison (d_costes, g_costes):\n",
    "    d_co = np.array(d_costes)\n",
    "    g_co = np.array(g_costes)\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    c = ax.plot(d_co)\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('Loss of Discriminator')\n",
    "    fig.savefig('Comparison_d.png')\n",
    "    fig1, ax1 = plt.subplots(figsize=(12,8))\n",
    "    c = ax1.plot(g_co)\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('Loss of Generator')\n",
    "    fig1.savefig('Comparison_g.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-107a00186727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#setup and hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "#setup and hyperparameters\n",
    "mnist = input_data.read_data_sets('data/')\n",
    "n_h, n_w, n_c = [32, 32, 1]\n",
    "n_z = 128\n",
    "n_batch = 100\n",
    "lr = 2.5e-4\n",
    "n_updates_total = 55000\n",
    "z_vis = noise(n_batch, n_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f3f543e99db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#tensorflow inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_z\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#setup modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#tensorflow inputs\n",
    "X = tf.placeholder(tf.float32, [None, n_h, n_w, n_c])\n",
    "Z = tf.placeholder(tf.float32, [None, n_z])\n",
    "\n",
    "#setup modules\n",
    "g_z   = generator(Z)\n",
    "d_g_z = discriminator(g_z)\n",
    "d_x   = discriminator(X, reuse=True)\n",
    "\n",
    "#generator tries to get discriminator to classify its samples as real\n",
    "g_loss     = cross_entropy_loss(logits=d_g_z, labels=tf.ones(tf.shape(d_g_z)))\n",
    "#discriminator tries to classify generator samples as fake\n",
    "d_g_z_loss = cross_entropy_loss(logits=d_g_z, labels=tf.zeros(tf.shape(d_g_z)))\n",
    "#discriminator tries to classify data samples as real\n",
    "d_x_loss   = cross_entropy_loss(logits=d_x,   labels=tf.ones(tf.shape(d_x)))\n",
    "#overall discriminator objective is correctly classifying both real and fake data\n",
    "d_loss     = (d_g_z_loss + d_x_loss)/2\n",
    "\n",
    "#build training ops\n",
    "g_vars  = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n",
    "g_train = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss,  var_list=g_vars)\n",
    "d_vars  = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "d_train = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss,  var_list=d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b52d5b0811f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mg_costes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"logs/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#alternate updates of discriminator on minibatches of real data and generator\n",
    "%matplotlib inline\n",
    "d_costes = list()\n",
    "g_costes = list()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for n_updates in tqdm(list(range(n_updates_total)), ncols=80, leave=False):\n",
    "    xs, _ = mnist.train.next_batch(n_batch)\n",
    "    d_cost, _ = sess.run([d_loss, d_train], {X:preprocess(xs), Z:noise(n_batch, n_z)})\n",
    "    g_cost, _ = sess.run([g_loss, g_train], {Z:noise(n_batch, n_z)})\n",
    "    d_costes.append(d_cost)\n",
    "    g_costes.append(g_cost)\n",
    "    if n_updates % 1000 == 0:\n",
    "        visualize(z_vis)\n",
    "\n",
    "PlotComparison (d_costes, g_costes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
